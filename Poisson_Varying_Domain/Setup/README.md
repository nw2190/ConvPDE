# Generating PDE Datasets

## Dataset Format
The datasets generated by the code provided in the `Setup/` directories for each example are organized into `mesh_*.npy`, `data_*.npy`, and `solutions_*.npy` files stored in the `Meshes/`, `Data/`, and `Solutions/` subdirectories, respectively.  The `Variable_Coefficient` problem setup generates additional `coeff_*.npy` terms which are also stored in the `Data/` subdirectory.

Each of the stored numpy arrays corresponds to a discretization of the continuous functions associated with a given PDE system and are formatted as fixed 128x128 resolution arrays.  The mesh files are encoded as integer-valued arrays with values of zero outside of the domain, values of one throughout the interior of the domain, and values of two along the boundary; these files are used to adapt the loss function to the specific domains in consideration during training.

The NumPy arrays are then preprocessed, consolidated, and stored in the TFRecord format in the `Setup/DATA/` subdirectories.  An 80/20 split of the dataset is used to create training/validation sets; the training and validation indices are stored in the `Setup/DATA/t_indices.npy` and `Setup/DATA/v_indices.npy` files, respectively.


    
## Generating the Dataset
Various dataset parameters (e.g. the total number of examples to create) are available in the `setup_flags.py` files.

The predefined settings assume that 8 CPUs are available.  The Python [multiprocessing](https://docs.python.org/2/library/multiprocessing.html) package is used to parallelize the dataset creation procedures, and the CPU count can be adjusted using the `--cpu_count` flag.

    
Once these settings have been specified, the datasets can be generated by running the `Compute_Cholesky_Factors.py` and  `CREATE_DATASET.sh` scripts:
```console
$ python Compute_Cholesky_Factors.py
    
 [ Generating Covariances ]
    
  Progress:  100.0%

    
$ ./CREATE_DATASET.sh
    
 [ Sampling Functions ]

  Progress:  100.0%


 [ Converting Functions ]

  Progress:  100.0%


 [ Generating Meshes ]

  Progress:  100.0%


 [ Solving Systems ]

  Progress:  100.0%


 [ Preprocessing Data ]

  Progress:  100.0%

    
 [ Writing TFRecords ]

  Progress:  100.0%


 [ Cleaning XML Files ]

  Progress:  100.0%
  
```


This will create four subdirectories containing the generated dataset files:
* `Cholesky/` - contains .npy files for the Cholesky factors of covariance matrices
* `Data/` - contains .xml and .npy files for source terms
* `Meshes/` - contains .xml and .npy files for meshes
* `Solutions/` - contains .npy files for solutions

along with the preprocessed dataset example written to TFRecords files in the `./DATA/` directory.

**Note:** The `Cholesky/` files can be reused for all five problem setups (e.g. using symbolic links).  




## File Overview

##### `--- setup_flags.py ---`
Provides options for controlling the data generation procedure.


##### `--- CREATE_DATASET.sh ---`
**Technical Note:** 
All `*.sh` files in this repository are Bash scripts and must be made executable using `chmod +x *.sh`.


Convenience bash script for automatically creating and processing the dataset.


##### `--- Compute_Cholesky_Factors.py ---`
Computes the Cholesky factors for the covariance matrices corresponding to Gaussian processes of various length-scales.  This script uses the Python multiprocessing package to parallelize the `generate_covariance()` calls defined in the `sample_gaussian.py` file.


##### `--- Generate_Samples.py ---`
Generates samples from the Gaussian process priors using the associated Cholesky factors.  This script uses the Python multiprocessing package to parallelize the `sample_gaussian()` calls defined in the `sample_gaussian.py` file.


##### `--- Convert_Samples.py ---` 
Converts the array formatted samples into the `.xml` format for FEniCS compatibility.  This script uses the Python multiprocessing package to parallelize the `fast_convert_samples()` calls defined in the `sample_gaussian.py` file.


##### `--- Generate_Meshes.py ---`
Generates meshes for randomized polygonal geometries.  This script uses the Python multiprocessing package to parallelize the `gen_mesh()` calls defined in the `mesh.py` file.


##### `--- Solve_Systems.py ---` 
Solves the associated PDE systems and converts solutions into TensorFlow compatible arrays.  This script uses the Python multiprocessing package to parallelize the `gen_soln()` calls defined in the `solver.py` file.


##### `--- Preprocess_Data.py ---`
Removes values outside of the domain and normalizes the source term and solution arrays.


##### `--- Write_TFRecords.py ---`
Writes training and validation data to `.tfrecord` protocol buffer files.


##### `--- Clean_XML.py ---`
Removes the FEniCS `.xml` files once solutions have been generated.


##### `--- setup_reader.py ---`
Provides functions for plotting data, meshes, and solutions.

